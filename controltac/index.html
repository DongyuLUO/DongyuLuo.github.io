<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Downstream Tasks</h2>

        <h3 class="subtitle is-5 has-text-centered mobile-margin">Force Estimation</h3>
        <div class="content has-text-justified mobile-padding">
          <p class="is-size-6 mobile-text mobile-margin">
            The left figure illustrates how the force-controlled component in <span class="name">ControlTac</span> augments 
            1,000 real samples with a larger set of generated tactile images, leading to a substantial reduction in MAE 
            compared to using real data alone. Notably, by incorporating the generated data, the model achieves comparable 
            performance to training on the full real dataset (20,000 images) using only 8,000 real samples. This demonstrates 
            that the generated data effectively enrich the force distribution at each contact position, thereby enhancing the 
            training of the force estimator. Moreover, combining a larger quantity of both real and generated data yields the 
            best overall performance, underscoring the realism and utility of the generated samples.
          </p>
          <p class="is-size-6 mobile-text mobile-margin">
            Building on the force-controlled component, we further integrate the position-controlled component of 
            <span class="name">ControlTac</span>. To highlight the importance of diverse contact positions in 
            training a robust force estimator, we divide the real dataset by contact angle, since tactile image 
            appearance varies across different contact angles. The right figure presents the MAE of force estimation 
            under different training conditions. The results show that incorporating position-controlled generation 
            effectively compensates for limited angular diversity in real data, significantly improving performance 
            even when only a small subset of real images is available—especially in scenarios where the real data covers 
            a narrow range of angles.
          </p>
          <figure class="image">
            <img src="./static/images/force1.png" alt="Force estimation results." class="responsive-image mobile-full-width">
          </figure>
          <p class="is-size-6 mobile-text mobile-margin">
            We further validate the effectiveness of <span class="name">ControlTac</span> in real-world pushing 
            experiments. The force estimator trained only with generated tactile data achieves
            comparable performance to the one trained on real tactile data, demonstrating that the generated data is 
            realistic and reliable enough to be used directly for training in practical scenarios.
          </p>
          <div class="gif-container">
            <img src="./static/images/force2.gif" alt="Real-world pushing experiments." class="responsive-image mobile-large tablet-large desktop-constrained">
          </div>
        </div>

        <h3 class="subtitle is-5 has-text-centered mobile-margin">Position Estimation</h3>
        <div class="content has-text-justified mobile-padding">
          <p class="is-size-6 mobile-text mobile-margin">
            As shown in the table below, pose estimators trained on tactile images generated by 
            <span class="name">ControlTac</span> achieve strong performance across all objects, 
            including the unseen T Shape. In particular, using a larger amount of generated data 
            leads to better results than using real data alone, as it is sufficiently realistic 
            and covers a much wider range of contact positions and forces. We also compare the 
            performance of the pose estimator using varying forces versus a fixed force (denoted 
            as fixed in the table below, where the fixed force is set to the median value of 6.5 N). 
            The results show that using varying force yields better performance, as contact force naturally 
            changes during inference.
          </p>
          <figure class="image">
            <img src="./static/images/pos1.png" alt="Pose estimation comparison." class="responsive-image mobile-large tablet-large" style="max-width: 40%; height: auto;">
          </figure>
          <p class="is-size-6 mobile-text mobile-margin">
            To further evaluate the performance of the pose estimator trained with <span class="name">ControlTac</span>-generated data, 
            we conducted a real-time pose tracking experiment. Our model successfully tracked poses at a 
            frequency of 10 Hz, highlighting its practicality in dynamic real-world scenarios.
          </p>
          <div class="gif-container">
            <img src="./static/images/tracking.gif" alt="Real-time pose tracking." class="responsive-image mobile-full-width">
          </div>
          <p class="is-size-6 mobile-text mobile-margin">
            In the Precise Insertion task, the pose estimator trained with <span class="name">ControlTac</span>-generated 
            data achieved success rates of 90% on the cylinder and 85% on the cross. Notably, 
            it also reached an 85% success rate on the unseen T-shape.
          </p>
          <div class="gif-container">
            <img src="./static/images/insertion.gif" alt="Precise insertion task results." class="responsive-image mobile-large tablet-large" style="max-width: 40%; height: auto;">
          </div>
        </div>

        <h3 class="subtitle is-5 has-text-centered mobile-margin">Object Classification</h3>
        <div class="content has-text-justified mobile-padding">
          <p class="is-size-6 mobile-text mobile-margin">
            In the object classification task, we found that compared to traditional augmentation methods, using <span class="name">ControlTac</span> for data augmentation yields significantly better performance—whether with a simple CNN classifier, a ViT trained from scratch, or a ViT pretrained on ImageNet.
          </p>
          <p class="is-size-6 mobile-text mobile-margin" style="font-style: italic;">
            Note: G = geometric augmentation; C = color augmentation; Gen = our <span class="name" style="color: black;">ControlTac</span>
-based augmentation method.
          </p>
          <figure class="image">
            <img src="./static/images/classification.png" alt="Object classification results." class="responsive-image mobile-full-width">
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content mobile-padding">
    <h2 class="title">BibTeX</h2>
    <pre style="font-size: 0.8rem; overflow-x: auto;"><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/DongyuLUO" class="external-link">
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content mobile-padding">
          <p style="margin-top: 1rem; font-style: italic;">
            Website template modified from <a href="https://nerfies.github.io/" target="_blank" rel="noopener noreferrer">NeRFies</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<script>
  // Navbar burger functionality for mobile
  document.addEventListener('DOMContentLoaded', () => {
    // Get all "navbar-burger" elements
    const navbarBurgers = Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'), 0);

    // Add a click event on each of them
    navbarBurgers.forEach(el => {
      el.addEventListener('click', () => {
        // Get the target from the "data-target" attribute
        const target = el.dataset.target;
        const targetElement = document.getElementById(target);

        // Toggle the "is-active" class on both the "navbar-burger" and the "navbar-menu"
        el.classList.toggle('is-active');
        targetElement.classList.toggle('is-active');
      });
    });
  });
</script>

</body>
</html>
